{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52ebc674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nithintata/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Python available modulses\n",
    "import glob\n",
    "import os\n",
    "import textract\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "# Developed Module\n",
    "import text_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "afdd7ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello.pdf\n",
      "Cpmpleted\n",
      "The role:&nbsp;Reporting to the Manager, Growth Engineering, the Front-End Web Developer will tie their abstract thinking with their technical skills to be a pillar of support within both their team and the Growth department. You'll use your skills to build &amp; test on our web properties while collaborating with key stakeholders to understand success on our sites, and use your creativity to balance our analytical mindset.&nbsp;The Front-End Web Developer will..&nbsp;• Translate design mockups from Figma into responsive HTML and CSS while adhering to best practices for performance, UX, SEO, and accessibility.&nbsp;• Update our marketing WordPress sites (getjobber.com and academy.getjobber.com). • Create microsites, web animations and interactive content.&nbsp;• Follow brand and design guidelines and bring ideas to collaborative design brainstorms&nbsp;• Work with stakeholders on other teams to understand key data metrics. To be successful, you must have:&nbsp;• 2-3+ years of experience with semantic HTML, CSS, Javascript, React, PHP in WordPress, and GIT.&nbsp;• To collaborate and listen. You're supportive and open to opinions outside of your own, have no problem asking others \"how can I help?\" Or \"can I get help?\" to clear roadblocks and build relationships, and can take feedback and incorporate it to make your work even better.&nbsp;• To be a solid communicator. You're adept at working with designers and clients to anticipate challenges, discussing the pros and cons of possible solutions, and deciding on a course of action as a group. • To know how to juggle. We're a dynamic start-up, and with that can come change — you need to be comfortable multitasking, prioritizing, and able to estimate/negotiate deadlines.&nbsp;• To be ambitious and humble. You don't accept the status quo simply because \"it's what we've always done.\" You push for better, newer, and more innovative ways to do things — all while staying humble.&nbsp;• To be data-focused. It's important to understand the impact of your work, you need to learn about our metrics and the impact of increasing or decreasing our conversion rates. That said, we need you to balance this mindset with user research and qualitative feedback. It would be really great (but not a deal-breaker)&nbsp;if you had…&nbsp;• A post-secondary degree…. • Experience with SCSS, NPM, Node.js or ParcelJS.&nbsp;• Experience with SEO. • A/B testing and data science experience.&nbsp;• Experience with Modern WordPress and Javascript (gutenberg).\n",
      "Path                                                sample.pdf\n",
      "File Name                                               sample\n",
      "Text         u l l   u e   u r   k l l     b u l   p   e   ...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>Resume</th>\n",
       "      <th>URLS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.20736</td>\n",
       "      <td>hello</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Distance Resume URLS\n",
       "1   1.20736  hello    h"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "resume_list1=[]\n",
    "# a=requests.get(\"https://firestore.googleapis.com/v1/projects/capestone-945f7/databases/(default)/documents/jobsApplied\").json()\n",
    "# for aa in a[\"documents\"]:\n",
    "#     resume_list1.append([aa[\"fields\"][\"url\"][\"stringValue\"],aa[\"fields\"][\"resumeData\"][\"mapValue\"][\"fields\"][\"name\"][\"stringValue\"].replace(\" \",\"-\")])\n",
    "\n",
    "# to extract data\n",
    "def extract_text_from_pdf(files_list):\n",
    "    resumes = [] # Stores final processed resume files \n",
    "    for pdf_path in files_list:\n",
    "        text = ''\n",
    "        with open(pdf_path, 'rb') as fh:\n",
    "            # iterate over all pages of PDF document\n",
    "            for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "                # creating a resoure manager\n",
    "                resource_manager = PDFResourceManager()\n",
    "\n",
    "                # create a file handle\n",
    "                fake_file_handle = StringIO()\n",
    "\n",
    "                # creating a text converter object\n",
    "                converter = TextConverter(\n",
    "                                    resource_manager, \n",
    "                                    fake_file_handle, \n",
    "                                    codec='utf-8', \n",
    "                                    laparams=LAParams()\n",
    "                            )\n",
    "\n",
    "                # creating a page interpreter\n",
    "                page_interpreter = PDFPageInterpreter(\n",
    "                                    resource_manager, \n",
    "                                    converter\n",
    "                                )\n",
    "\n",
    "                # process current page\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "                # extract text\n",
    "                text += fake_file_handle.getvalue()\n",
    "                text = text.replace('\\n', ' ')\n",
    "                \n",
    "                # close open handles\n",
    "                converter.close()\n",
    "                fake_file_handle.close()\n",
    "            resumes.append(text_process.normalize(text))\n",
    "            \n",
    "    df = {'Path':resume_list, 'File Name': resume_list[0].split('.')[0], 'Text':resumes, 'urls' : resume_urls}\n",
    "    #print(resume_list,file_names,resumes,resume_urls)\n",
    "    data = pd.DataFrame(df)\n",
    "    return data\n",
    "\n",
    "#parsing job description\n",
    "\n",
    "file_loc = '/Users/nithintata/Documents/GitHub/Capestone-project-group3/Original_Resumes/'\n",
    "def parsing_jd(jd_file_name):\n",
    "#     path = file_loc + jd_file_name + '.txt'\n",
    "#     for file in glob.glob(path, recursive=True):\n",
    "#         if not file in job_desc_files: \n",
    "#             job_desc_files.append(file)\n",
    "#     with open(path, 'rt') as file:\n",
    "#         jd = file.read()\n",
    "    jd=abc\n",
    "    print(jd)\n",
    "    jd = summarize(jd, word_count=200)\n",
    "    #file.close()\n",
    "    jd = text_process.normalize(jd)  \n",
    "    df = pd.DataFrame(columns=['Path', 'File Name', 'Text'])\n",
    "    #df.loc[0] = [path, jd_file_name, jd]\n",
    "    df.loc[0] = ['sample.pdf', 'sample', jd]\n",
    "    print( df.loc[0] )\n",
    "    return df\n",
    "\n",
    "\n",
    "#calling function\n",
    "def resume_df(files_list, jd_file_name):\n",
    "    \n",
    "    df1 = extract_text_from_pdf(files_list)\n",
    "    df2 = parsing_jd(jd_file_name)\n",
    "    df3 = pd.concat([df1, df2], ignore_index = True)\n",
    "    \n",
    "    tfidfVect = TfidfVectorizer()\n",
    "    tfidf = tfidfVect.fit_transform(df3['Text'])\n",
    "    job_desc = df3[df3['File Name'] == jd_file_name]\n",
    "    \n",
    "    jd_tfidfVect = TfidfVectorizer()\n",
    "    jd_tfidfVect = jd_tfidfVect.fit(df3['Text'])\n",
    "    jd_tfidf = jd_tfidfVect.transform(job_desc['Text'])\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(tfidf)\n",
    "    distances, indices = nbrs.kneighbors(jd_tfidf)\n",
    "    names_similar = pd.Series(indices.flatten()).map(df3.reset_index()['File Name'])\n",
    "    similar_urls = pd.Series(indices.flatten()).map(df3.reset_index()['urls'])\n",
    "    result = pd.DataFrame({'Distance':distances.flatten(), 'Resume':names_similar, 'URLS' : similar_urls})\n",
    "    \n",
    "    return result[1:]\n",
    "fine_name=\"\"\n",
    "resume_list=[]\n",
    "def pre_process():\n",
    "    global fine_name\n",
    "    global resume_list\n",
    "    resume_list1=[\"https://firebasestorage.googleapis.com/v0/b/capestone-945f7.appspot.com/o/0q1yyOoSTOQKSJt1yDeh%2FNithin_Tataundefined?alt=media&token=66cdc9a4-0ce7-4dc2-b7ab-0c3040e9a8fa\"]   \n",
    "    resume_urls=[]\n",
    "    for i in resume_list1:\n",
    "        resume_urls.append(i[0])\n",
    "    #resume_list\n",
    "    import tempfile\n",
    "    for file_url in zip(resume_list1,range(0,len(resume_list1))):\n",
    "        response=requests.get(file_url[0])\n",
    "        #fine_name=os.path.join(tempfile.gettempdir(),\"hello\"+str(file_url[1]))\n",
    "        fine_name=str(\"hello.pdf\")\n",
    "        print(fine_name)\n",
    "        with open(fine_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Cpmpleted\")\n",
    "\n",
    "\n",
    "    resume_list = [] # stores all resumes\n",
    "    resume_list_pdf = [] # Captures files with pdf extension\n",
    "    resume_list_doc = [] # Captures files with doc extension\n",
    "    resume_list_docx = [] # Captures files with docx extension\n",
    "\n",
    "    file_names = [] # STORES RESUME FILE NAMES\n",
    "    job_desc_files = [] # stores jd paths\n",
    "\n",
    "    for i in resume_list1:\n",
    "        resume_list.append(\"hello.pdf\")\n",
    "    return resume_list                                   \n",
    "\n",
    "pre= pre_process()                                     \n",
    "ss=resume_df(resume_list, 'sample')\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "896fceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=requests.get(\"https://firestore.googleapis.com/v1/projects/capestone-945f7/databases/(default)/documents/jobsApplied\").json()\n",
    "abc=a[\"documents\"][0][\"fields\"][\"jobDesc\"][\"stringValue\"].replace(\"</p>\",\"\").replace(\"<p>\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddc4e75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The role:&nbsp;Reporting to the Manager, Growth Engineering, the Front-End Web Developer will tie their abstract thinking with their technical skills to be a pillar of support within both their team and the Growth department. You\\'ll use your skills to build &amp; test on our web properties while collaborating with key stakeholders to understand success on our sites, and use your creativity to balance our analytical mindset.&nbsp;The Front-End Web Developer will..&nbsp;• Translate design mockups from Figma into responsive HTML and CSS while adhering to best practices for performance, UX, SEO, and accessibility.&nbsp;• Update our marketing WordPress sites (getjobber.com and academy.getjobber.com). • Create microsites, web animations and interactive content.&nbsp;• Follow brand and design guidelines and bring ideas to collaborative design brainstorms&nbsp;• Work with stakeholders on other teams to understand key data metrics. To be successful, you must have:&nbsp;• 2-3+ years of experience with semantic HTML, CSS, Javascript, React, PHP in WordPress, and GIT.&nbsp;• To collaborate and listen. You\\'re supportive and open to opinions outside of your own, have no problem asking others \"how can I help?\" Or \"can I get help?\" to clear roadblocks and build relationships, and can take feedback and incorporate it to make your work even better.&nbsp;• To be a solid communicator. You\\'re adept at working with designers and clients to anticipate challenges, discussing the pros and cons of possible solutions, and deciding on a course of action as a group. • To know how to juggle. We\\'re a dynamic start-up, and with that can come change — you need to be comfortable multitasking, prioritizing, and able to estimate/negotiate deadlines.&nbsp;• To be ambitious and humble. You don\\'t accept the status quo simply because \"it\\'s what we\\'ve always done.\" You push for better, newer, and more innovative ways to do things — all while staying humble.&nbsp;• To be data-focused. It\\'s important to understand the impact of your work, you need to learn about our metrics and the impact of increasing or decreasing our conversion rates. That said, we need you to balance this mindset with user research and qualitative feedback. It would be really great (but not a deal-breaker)&nbsp;if you had…&nbsp;• A post-secondary degree…. • Experience with SCSS, NPM, Node.js or ParcelJS.&nbsp;• Experience with SEO. • A/B testing and data science experience.&nbsp;• Experience with Modern WordPress and Javascript (gutenberg).'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd22ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python available modulses\n",
    "import glob\n",
    "import os\n",
    "os.system(\"python -m spacy download en_core_web_sm\")\n",
    "import textract\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import spacy\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        # print(word)\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = stem_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# to extract data\n",
    "def extract_text_from_pdf(files_list):\n",
    "    resumes = [] # Stores final processed resume files \n",
    "    for pdf_path in files_list:\n",
    "        text = ''\n",
    "        with open(pdf_path, 'rb') as fh:\n",
    "            # iterate over all pages of PDF document\n",
    "            for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "                # creating a resoure manager\n",
    "                resource_manager = PDFResourceManager()\n",
    "\n",
    "                # create a file handle\n",
    "                fake_file_handle = StringIO()\n",
    "\n",
    "                # creating a text converter object\n",
    "                converter = TextConverter(\n",
    "                                    resource_manager, \n",
    "                                    fake_file_handle, \n",
    "                                    codec='utf-8', \n",
    "                                    laparams=LAParams()\n",
    "                            )\n",
    "\n",
    "                # creating a page interpreter\n",
    "                page_interpreter = PDFPageInterpreter(\n",
    "                                    resource_manager, \n",
    "                                    converter\n",
    "                                )\n",
    "\n",
    "                # process current page\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "                # extract text\n",
    "                text += fake_file_handle.getvalue()\n",
    "                text = text.replace('\\n', ' ')\n",
    "                \n",
    "                # close open handles\n",
    "                converter.close()\n",
    "                fake_file_handle.close()\n",
    "            resumes.append(normalize(text))\n",
    "            \n",
    "    for name in resume_list:\n",
    "        temp = name.split('.')[0]\n",
    "        temp = temp.split('/')[-1]\n",
    "        file_names.append(temp)\n",
    "    df = {'Path':resume_list, 'File Name': file_names, 'Text':resumes, 'urls' : resume_urls}\n",
    "    data = pd.DataFrame(df)\n",
    "    return data\n",
    "\n",
    "#parsing job description\n",
    "\n",
    "file_loc = '/Users/nithintata/Documents/GitHub/Capestone-project-group3/Original_Resumes/'\n",
    "def parsing_jd(jd_file_name):\n",
    "    path = file_loc + jd_file_name + '.txt'\n",
    "    for file in glob.glob(path, recursive=True):\n",
    "        if not file in job_desc_files: \n",
    "            job_desc_files.append(file)\n",
    "    with open(path, 'rt') as file:\n",
    "        jd = file.read()\n",
    "    jd = summarize(jd, word_count=200)\n",
    "    file.close()\n",
    "    jd = normalize(jd)  \n",
    "    df = pd.DataFrame(columns=['Path', 'File Name', 'Text'])\n",
    "    df.loc[0] = [path, jd_file_name, jd]\n",
    "    return df\n",
    "\n",
    "\n",
    "#calling function\n",
    "def resume_df(files_list, jd_file_name):\n",
    "    \n",
    "    df1 = extract_text_from_pdf(files_list)\n",
    "    df2 = parsing_jd(jd_file_name)\n",
    "    df3 = pd.concat([df1, df2], ignore_index = True)\n",
    "    \n",
    "    tfidfVect = TfidfVectorizer()\n",
    "    tfidf = tfidfVect.fit_transform(df3['Text'])\n",
    "    job_desc = df3[df3['File Name'] == jd_file_name]\n",
    "    \n",
    "    jd_tfidfVect = TfidfVectorizer()\n",
    "    jd_tfidfVect = jd_tfidfVect.fit(df3['Text'])\n",
    "    jd_tfidf = jd_tfidfVect.transform(job_desc['Text'])\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=5).fit(tfidf)\n",
    "    distances, indices = nbrs.kneighbors(jd_tfidf)\n",
    "    names_similar = pd.Series(indices.flatten()).map(df3.reset_index()['File Name'])\n",
    "    similar_urls = pd.Series(indices.flatten()).map(df3.reset_index()['urls'])\n",
    "    result = pd.DataFrame({'Distance':distances.flatten(), 'Resume':names_similar, 'URLS' : similar_urls})\n",
    "    \n",
    "    return result[1:]\n",
    "\n",
    "# Developed Module\n",
    "#import text_process\n",
    "\n",
    "import requests\n",
    "\n",
    "resume_list1=[]\n",
    "# a=requests.get(\"https://firestore.googleapis.com/v1/projects/capestone-945f7/databases/(default)/documents/jobsApplied\").json()\n",
    "# for aa in a[\"documents\"]:\n",
    "#     resume_list1.append([aa[\"fields\"][\"url\"][\"stringValue\"],aa[\"fields\"][\"resumeData\"][\"mapValue\"][\"fields\"][\"name\"][\"stringValue\"].replace(\" \",\"-\")])\n",
    "resume_urls=[]\n",
    "# for i in resume_list1:\n",
    "#     resume_urls.append(i[0])\n",
    "#resume_list\n",
    "# resumeUrl = \"\"\n",
    "# resume_list1=[\"https://firebasestorage.googleapis.com/v0/b/capestone-945f7.appspot.com/o/0q1yyOoSTOQKSJt1yDeh%2FNithin_Tataundefined?alt=media&token=66cdc9a4-0ce7-4dc2-b7ab-0c3040e9a8fa\"]\n",
    "# resume_list1 = [resumeUrl]\n",
    "\n",
    "    \n",
    "    \n",
    "resume_list = [] # stores all resumes\n",
    "resume_list_pdf = [] # Captures files with pdf extension\n",
    "resume_list_doc = [] # Captures files with doc extension\n",
    "resume_list_docx = [] # Captures files with docx extension\n",
    " \n",
    "file_names = [] # STORES RESUME FILE NAMES\n",
    "job_desc_files = [] # stores jd paths\n",
    "fine_name=\"\"\n",
    "for i in resume_list1:\n",
    "    resume_list.append(str(i[1])+\".pdf\")\n",
    "def hello_firestore(event, context):\n",
    "    \"\"\"Triggered by a change to a Firestore document.\n",
    "    Args:\n",
    "         event (dict): Event payload.\n",
    "         context (google.cloud.functions.Context): Metadata for the event.\n",
    "    \"\"\"\n",
    "    global resume_list1\n",
    "    global resumeUrl\n",
    "    global resume_urls\n",
    "    #resumeUrl = event[\"value\"]['fields'][\"url\"]['stringValue']\n",
    "    #resume_urls= [event[\"value\"]['fields'][\"url\"]['stringValue']]\n",
    "    #resume_list1 = [event[\"value\"]['fields'][\"url\"]['stringValue']]\n",
    "    resumeUrl=\"https://firebasestorage.googleapis.com/v0/b/capestone-945f7.appspot.com/o/0q1yyOoSTOQKSJt1yDeh%2FNithin_Tataundefined?alt=media&token=66cdc9a4-0ce7-4dc2-b7ab-0c3040e9a8fa\"\n",
    "    resume_urls=[\"https://firebasestorage.googleapis.com/v0/b/capestone-945f7.appspot.com/o/0q1yyOoSTOQKSJt1yDeh%2FNithin_Tataundefined?alt=media&token=66cdc9a4-0ce7-4dc2-b7ab-0c3040e9a8fa\"]\n",
    "    resume_list1=[\"https://firebasestorage.googleapis.com/v0/b/capestone-945f7.appspot.com/o/0q1yyOoSTOQKSJt1yDeh%2FNithin_Tataundefined?alt=media&token=66cdc9a4-0ce7-4dc2-b7ab-0c3040e9a8fa\"]\n",
    "    import tempfile\n",
    "    global fine_name\n",
    "    for files_url in zip(resume_list1,range(0,len(resume_list1))):\n",
    "        response=requests.get(files_url[0][0])\n",
    "        fine_name=os.path.join(tempfile.gettempdir(),\"hello.pdf\")\n",
    "        #fine_name=str(file_url[0][1].replace(\" \",\"_\"))+\".pdf\"\n",
    "        print(fine_name)\n",
    "        with open(fine_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Cpmpleted\")\n",
    "    ss=resume_df(resume_list, 'sample')\n",
    "    print(ss)\n",
    "    path_parts = context.resource.split('/documents/')[1].split('/')\n",
    "    collection_path = path_parts[0]\n",
    "    document_path = '/'.join(path_parts[1:])\n",
    "\n",
    "    affected_doc = client.collection(collection_path).document(document_path)\n",
    "    affected_doc.update({\n",
    "            u'resumeValue':ss[\"Distance\"]\n",
    "        })\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534c8412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db93dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resume_list1=[]\n",
    "a=requests.get(\"https://firestore.googleapis.com/v1/projects/capestone-945f7/databases/(default)/documents/jobsApplied\").json()\n",
    "for aa in a[\"documents\"]:\n",
    "    resume_list1.append([aa[\"fields\"][\"url\"][\"stringValue\"],aa[\"fields\"][\"resumeData\"][\"mapValue\"][\"fields\"][\"name\"][\"stringValue\"].replace(\" \",\"-\")])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5846976e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://firebasestorage.googleapis.com/v0/b/capestone-945f7.appspot.com/o/0q1yyOoSTOQKSJt1yDeh%2FJaswanth%20TataFinal_CV.pdf?alt=media&token=9b192e75-9c27-4574-bf11-6d7f7ccc743d',\n",
       " '\\u200b-Github']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37717395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://firebasestorage.googleapis.com/v0/b/capestone-945f7.appspot.com/o/eyBbM03W6x2ckvJemmjg%2FNithin%20Tatafront-end%20dev%20resume%20sample.pdf?alt=media&token=e5221fdc-4f57-45fc-b0c2-c6efe2b03e69'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss[\"URLS\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a94f61ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>Resume</th>\n",
       "      <th>URLS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.859663</td>\n",
       "      <td>​-Github</td>\n",
       "      <td>https://firebasestorage.googleapis.com/v0/b/ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.897217</td>\n",
       "      <td>Jalpa-Dave</td>\n",
       "      <td>https://firebasestorage.googleapis.com/v0/b/ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.973399</td>\n",
       "      <td>Viktor-Amundsen</td>\n",
       "      <td>https://firebasestorage.googleapis.com/v0/b/ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.973399</td>\n",
       "      <td>Viktor-Amundsen</td>\n",
       "      <td>https://firebasestorage.googleapis.com/v0/b/ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Distance           Resume  \\\n",
       "1  0.859663         ​-Github   \n",
       "2  0.897217       Jalpa-Dave   \n",
       "3  0.973399  Viktor-Amundsen   \n",
       "4  0.973399  Viktor-Amundsen   \n",
       "\n",
       "                                                URLS  \n",
       "1  https://firebasestorage.googleapis.com/v0/b/ca...  \n",
       "2  https://firebasestorage.googleapis.com/v0/b/ca...  \n",
       "3  https://firebasestorage.googleapis.com/v0/b/ca...  \n",
       "4  https://firebasestorage.googleapis.com/v0/b/ca...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "172b63d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f29050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resume_list1=[]\n",
    "# a=requests.get(\"https://firestore.googleapis.com/v1/projects/capestone-945f7/databases/(default)/documents/jobsApplied\").json()\n",
    "# for aa in a[\"documents\"]:\n",
    "#     resume_list1.append([aa[\"fields\"][\"url\"][\"stringValue\"],aa[\"fields\"][\"resumeData\"][\"mapValue\"][\"fields\"][\"name\"][\"stringValue\"].replace(\" \",\"-\")])\n",
    "\n",
    "# to extract data\n",
    "def extract_text_from_pdf(files_list):\n",
    "    resumes = [] # Stores final processed resume files \n",
    "    for pdf_path in files_list:\n",
    "        text = ''\n",
    "        with open(pdf_path, 'rb') as fh:\n",
    "            # iterate over all pages of PDF document\n",
    "            for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "                # creating a resoure manager\n",
    "                resource_manager = PDFResourceManager()\n",
    "\n",
    "                # create a file handle\n",
    "                fake_file_handle = StringIO()\n",
    "\n",
    "                # creating a text converter object\n",
    "                converter = TextConverter(\n",
    "                                    resource_manager, \n",
    "                                    fake_file_handle, \n",
    "                                    codec='utf-8', \n",
    "                                    laparams=LAParams()\n",
    "                            )\n",
    "\n",
    "                # creating a page interpreter\n",
    "                page_interpreter = PDFPageInterpreter(\n",
    "                                    resource_manager, \n",
    "                                    converter\n",
    "                                )\n",
    "\n",
    "                # process current page\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "                # extract text\n",
    "                text += fake_file_handle.getvalue()\n",
    "                text = text.replace('\\n', ' ')\n",
    "                \n",
    "                # close open handles\n",
    "                converter.close()\n",
    "                fake_file_handle.close()\n",
    "            resumes.append(text_process.normalize(text))\n",
    "            \n",
    "    df = {'Path':resume_list, 'File Name': resume_list[0].split('.')[0], 'Text':resumes, 'urls' : resume_urls}\n",
    "    #print(resume_list,file_names,resumes,resume_urls)\n",
    "    data = pd.DataFrame(df)\n",
    "    return data\n",
    "\n",
    "#parsing job description\n",
    "\n",
    "file_loc = '/Users/nithintata/Documents/GitHub/Capestone-project-group3/Original_Resumes/'\n",
    "def parsing_jd(jd_file_name):\n",
    "    path = file_loc + jd_file_name + '.txt'\n",
    "    for file in glob.glob(path, recursive=True):\n",
    "        if not file in job_desc_files: \n",
    "            job_desc_files.append(file)\n",
    "    with open(path, 'rt') as file:\n",
    "        jd = file.read()\n",
    "    jd = summarize(jd, word_count=200)\n",
    "    file.close()\n",
    "    jd = text_process.normalize(jd)  \n",
    "    df = pd.DataFrame(columns=['Path', 'File Name', 'Text'])\n",
    "    df.loc[0] = [path, jd_file_name, jd]\n",
    "    return df\n",
    "\n",
    "\n",
    "#calling function\n",
    "def resume_df(files_list, jd_file_name):\n",
    "    \n",
    "    df1 = extract_text_from_pdf(files_list)\n",
    "    df2 = parsing_jd(jd_file_name)\n",
    "    df3 = pd.concat([df1, df2], ignore_index = True)\n",
    "    \n",
    "    tfidfVect = TfidfVectorizer()\n",
    "    tfidf = tfidfVect.fit_transform(df3['Text'])\n",
    "    job_desc = df3[df3['File Name'] == jd_file_name]\n",
    "    \n",
    "    jd_tfidfVect = TfidfVectorizer()\n",
    "    jd_tfidfVect = jd_tfidfVect.fit(df3['Text'])\n",
    "    jd_tfidf = jd_tfidfVect.transform(job_desc['Text'])\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(tfidf)\n",
    "    distances, indices = nbrs.kneighbors(jd_tfidf)\n",
    "    names_similar = pd.Series(indices.flatten()).map(df3.reset_index()['File Name'])\n",
    "    similar_urls = pd.Series(indices.flatten()).map(df3.reset_index()['urls'])\n",
    "    result = pd.DataFrame({'Distance':distances.flatten(), 'Resume':names_similar, 'URLS' : similar_urls})\n",
    "    \n",
    "    return result[1:]\n",
    "fine_name=\"\"\n",
    "resume_list=[]\n",
    "def pre_process(event):\n",
    "    global fine_name\n",
    "    global resume_list\n",
    "    #resume_list1=[\"https://firebasestorage.googleapis.com/v0/b/capestone-945f7.appspot.com/o/0q1yyOoSTOQKSJt1yDeh%2FNithin_Tataundefined?alt=media&token=66cdc9a4-0ce7-4dc2-b7ab-0c3040e9a8fa\"]   \n",
    "    resume_list1=[event[\"value\"]['fields'][\"url\"]['stringValue']]\n",
    "    resume_urls=[]\n",
    "    for i in resume_list1:\n",
    "        resume_urls.append(i[0])\n",
    "    #resume_list\n",
    "    import tempfile\n",
    "    for file_url in zip(resume_list1,range(0,len(resume_list1))):\n",
    "        response=requests.get(file_url[0])\n",
    "        #fine_name=os.path.join(tempfile.gettempdir(),\"hello\"+str(file_url[1]))\n",
    "        fine_name=str(\"hello.pdf\")\n",
    "        print(fine_name)\n",
    "        with open(fine_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Cpmpleted\")\n",
    "\n",
    "\n",
    "    resume_list = [] # stores all resumes\n",
    "    resume_list_pdf = [] # Captures files with pdf extension\n",
    "    resume_list_doc = [] # Captures files with doc extension\n",
    "    resume_list_docx = [] # Captures files with docx extension\n",
    "\n",
    "    file_names = [] # STORES RESUME FILE NAMES\n",
    "    job_desc_files = [] # stores jd paths\n",
    "\n",
    "    for i in resume_list1:\n",
    "        resume_list.append(\"hello.pdf\")\n",
    "    return resume_list                                   \n",
    "\n",
    "\n",
    "def hello_firestore(event, context):\n",
    "    pre= pre_process(event)                                     \n",
    "    ss=resume_df(resume_list, 'sample')\n",
    "    path_parts = context.resource.split('/documents/')[1].split('/')\n",
    "    collection_path = path_parts[0]\n",
    "    document_path = '/'.join(path_parts[1:])\n",
    "    affected_doc = client.collection(collection_path).document(document_path)\n",
    "    affected_doc.update({\n",
    "            u'resumeValue':ss[\"Distance\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resume_list1=[]\n",
    "# a=requests.get(\"https://firestore.googleapis.com/v1/projects/capestone-945f7/databases/(default)/documents/jobsApplied\").json()\n",
    "# for aa in a[\"documents\"]:\n",
    "#     resume_list1.append([aa[\"fields\"][\"url\"][\"stringValue\"],aa[\"fields\"][\"resumeData\"][\"mapValue\"][\"fields\"][\"name\"][\"stringValue\"].replace(\" \",\"-\")])\n",
    "\n",
    "# to extract data\n",
    "def extract_text_from_pdf(files_list):\n",
    "    resumes = [] # Stores final processed resume files \n",
    "    for pdf_path in files_list:\n",
    "        text = ''\n",
    "        with open(pdf_path, 'rb') as fh:\n",
    "            # iterate over all pages of PDF document\n",
    "            for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "                # creating a resoure manager\n",
    "                resource_manager = PDFResourceManager()\n",
    "\n",
    "                # create a file handle\n",
    "                fake_file_handle = StringIO()\n",
    "\n",
    "                # creating a text converter object\n",
    "                converter = TextConverter(\n",
    "                                    resource_manager, \n",
    "                                    fake_file_handle, \n",
    "                                    codec='utf-8', \n",
    "                                    laparams=LAParams()\n",
    "                            )\n",
    "\n",
    "                # creating a page interpreter\n",
    "                page_interpreter = PDFPageInterpreter(\n",
    "                                    resource_manager, \n",
    "                                    converter\n",
    "                                )\n",
    "\n",
    "                # process current page\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "                # extract text\n",
    "                text += fake_file_handle.getvalue()\n",
    "                text = text.replace('\\n', ' ')\n",
    "                \n",
    "                # close open handles\n",
    "                converter.close()\n",
    "                fake_file_handle.close()\n",
    "            resumes.append(text_process.normalize(text))\n",
    "            \n",
    "    df = {'Path':resume_list, 'File Name': resume_list[0].split('.')[0], 'Text':resumes, 'urls' : resume_urls}\n",
    "    #print(resume_list,file_names,resumes,resume_urls)\n",
    "    data = pd.DataFrame(df)\n",
    "    return data\n",
    "\n",
    "#parsing job description\n",
    "\n",
    "file_loc = '/Users/nithintata/Documents/GitHub/Capestone-project-group3/Original_Resumes/'\n",
    "def parsing_jd(event):\n",
    "#     path = file_loc + jd_file_name + '.txt'\n",
    "#     for file in glob.glob(path, recursive=True):\n",
    "#         if not file in job_desc_files: \n",
    "#             job_desc_files.append(file)\n",
    "#     with open(path, 'rt') as file:\n",
    "#         jd = file.read()\n",
    "#     jd = summarize(jd, word_count=200)\n",
    "#     file.close()\n",
    "    jd=event[\"value\"]\n",
    "    jd = text_process.normalize(jd)  \n",
    "    df = pd.DataFrame(columns=['Path', 'File Name', 'Text'])\n",
    "    #df.loc[0] = [path, jd_file_name, jd]\n",
    "    df.loc[0] = ['hello.pdf', 'hello', jd]\n",
    "    return df\n",
    "\n",
    "\n",
    "#calling function\n",
    "def resume_df(files_list, jd_file_name, event):\n",
    "    \n",
    "    df1 = extract_text_from_pdf(files_list)\n",
    "    df2 = parsing_jd(event)\n",
    "    df3 = pd.concat([df1, df2], ignore_index = True)\n",
    "    \n",
    "    tfidfVect = TfidfVectorizer()\n",
    "    tfidf = tfidfVect.fit_transform(df3['Text'])\n",
    "    job_desc = df3[df3['File Name'] == jd_file_name]\n",
    "    \n",
    "    jd_tfidfVect = TfidfVectorizer()\n",
    "    jd_tfidfVect = jd_tfidfVect.fit(df3['Text'])\n",
    "    jd_tfidf = jd_tfidfVect.transform(job_desc['Text'])\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(tfidf)\n",
    "    distances, indices = nbrs.kneighbors(jd_tfidf)\n",
    "    names_similar = pd.Series(indices.flatten()).map(df3.reset_index()['File Name'])\n",
    "    similar_urls = pd.Series(indices.flatten()).map(df3.reset_index()['urls'])\n",
    "    result = pd.DataFrame({'Distance':distances.flatten(), 'Resume':names_similar, 'URLS' : similar_urls})\n",
    "    \n",
    "    return result[1:]\n",
    "fine_name=\"\"\n",
    "resume_list=[]\n",
    "def pre_process(event):\n",
    "    global fine_name\n",
    "    global resume_list\n",
    "    #resume_list1=[\"https://firebasestorage.googleapis.com/v0/b/capestone-945f7.appspot.com/o/0q1yyOoSTOQKSJt1yDeh%2FNithin_Tataundefined?alt=media&token=66cdc9a4-0ce7-4dc2-b7ab-0c3040e9a8fa\"]   \n",
    "    resume_list1=[event[\"value\"]['fields'][\"url\"]['stringValue']]\n",
    "    resume_urls=[]\n",
    "    for i in resume_list1:\n",
    "        resume_urls.append(i[0])\n",
    "    #resume_list\n",
    "    import tempfile\n",
    "    for file_url in zip(resume_list1,range(0,len(resume_list1))):\n",
    "        response=requests.get(file_url[0])\n",
    "        #fine_name=os.path.join(tempfile.gettempdir(),\"hello\"+str(file_url[1]))\n",
    "        fine_name=str(\"hello.pdf\")\n",
    "        print(fine_name)\n",
    "        with open(fine_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Cpmpleted\")\n",
    "\n",
    "\n",
    "    resume_list = [] # stores all resumes\n",
    "    resume_list_pdf = [] # Captures files with pdf extension\n",
    "    resume_list_doc = [] # Captures files with doc extension\n",
    "    resume_list_docx = [] # Captures files with docx extension\n",
    "\n",
    "    file_names = [] # STORES RESUME FILE NAMES\n",
    "    job_desc_files = [] # stores jd paths\n",
    "\n",
    "    for i in resume_list1:\n",
    "        resume_list.append(\"hello.pdf\")\n",
    "    return resume_list                                   \n",
    "\n",
    "\n",
    "def hello_firestore(event, context):\n",
    "    pre= pre_process(event)                                     \n",
    "    ss=resume_df(resume_list, 'sample',event)\n",
    "    path_parts = context.resource.split('/documents/')[1].split('/')\n",
    "    collection_path = path_parts[0]\n",
    "    document_path = '/'.join(path_parts[1:])\n",
    "    affected_doc = client.collection(collection_path).document(document_path)\n",
    "    affected_doc.update({\n",
    "            u'resumeValue':ss[\"Distance\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d07b030a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nithintata/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/bw/j7q1ndw51j399blq9n4d7wk80000gn/T/hello.pdf\n",
      "Cpmpleted\n"
     ]
    }
   ],
   "source": [
    "#final \n",
    "\n",
    "import requests\n",
    "import os\n",
    "# Python available modulses\n",
    "import glob\n",
    "import os\n",
    "os.system(\"python -m spacy download en_core_web_sm\")\n",
    "import textract\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import math\n",
    "import tempfile\n",
    "resume_list1=[]\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        # print(word)\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    #words = remove_stopwords(words)\n",
    "    words = stem_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return \" \".join(words)\n",
    "def extract_text_from_pdf(files_list):\n",
    "    resumes = [] # Stores final processed resume files \n",
    "    for pdf_path in files_list:\n",
    "        text = ''\n",
    "        import tempfile\n",
    "        pdf_path=os.path.join(tempfile.gettempdir(),\"hello.pdf\")\n",
    "        print(pdf_path)\n",
    "        with open(pdf_path, 'rb') as fh:\n",
    "            # iterate over all pages of PDF document\n",
    "            for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "                # creating a resoure manager\n",
    "                resource_manager = PDFResourceManager()\n",
    "\n",
    "                # create a file handle\n",
    "                fake_file_handle = StringIO()\n",
    "\n",
    "                # creating a text converter object\n",
    "                converter = TextConverter(\n",
    "                                    resource_manager, \n",
    "                                    fake_file_handle, \n",
    "                                    codec='utf-8', \n",
    "                                    laparams=LAParams()\n",
    "                            )\n",
    "\n",
    "                # creating a page interpreter\n",
    "                page_interpreter = PDFPageInterpreter(\n",
    "                                    resource_manager, \n",
    "                                    converter\n",
    "                                )\n",
    "\n",
    "                # process current page\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "                # extract text\n",
    "                text += fake_file_handle.getvalue()\n",
    "                text = text.replace('\\n', ' ')\n",
    "                \n",
    "                # close open handles\n",
    "                converter.close()\n",
    "                fake_file_handle.close()\n",
    "            resumes.append(normalize(text))\n",
    "            \n",
    "    df = {'Path':resume_list, 'File Name': resume_list[0].split('.')[0], 'Text':resumes, 'urls' : \"hello.pdf\"}\n",
    "    #print(resume_list,file_names,resumes,resume_urls)\n",
    "    data = pd.DataFrame(df)\n",
    "    return data\n",
    "\n",
    "#parsing job description\n",
    "\n",
    "file_loc = '/Users/nithintata/Documents/GitHub/Capestone-project-group3/Original_Resumes/'\n",
    "def parsing_jd(event):\n",
    "#     path = file_loc + jd_file_name + '.txt'\n",
    "#     for file in glob.glob(path, recursive=True):\n",
    "#         if not file in job_desc_files: \n",
    "#             job_desc_files.append(file)\n",
    "#     with open(path, 'rt') as file:\n",
    "#         jd = file.read()\n",
    "#     jd = summarize(jd, word_count=200)\n",
    "#     file.close()\n",
    "    jd=event[\"value\"]\n",
    "    jd = normalize(jd)  \n",
    "    df = pd.DataFrame(columns=['Path', 'File Name', 'Text'])\n",
    "    #df.loc[0] = [path, jd_file_name, jd]\n",
    "    df.loc[0] = ['hello.pdf', 'hello', jd]\n",
    "    return df\n",
    "\n",
    "\n",
    "#calling function\n",
    "def resume_df(files_list, jd_file_name, event):\n",
    "    \n",
    "    df1 = extract_text_from_pdf(files_list)\n",
    "    df2 = parsing_jd(event)\n",
    "    df3 = pd.concat([df1, df2], ignore_index = True)\n",
    "    \n",
    "    tfidfVect = TfidfVectorizer()\n",
    "    tfidf = tfidfVect.fit_transform(df3['Text'])\n",
    "    job_desc = df3[df3['File Name'] == jd_file_name]\n",
    "    \n",
    "    jd_tfidfVect = TfidfVectorizer()\n",
    "    jd_tfidfVect = jd_tfidfVect.fit(df3['Text'])\n",
    "    jd_tfidf = jd_tfidfVect.transform(job_desc['Text'])\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(tfidf)\n",
    "    distances, indices = nbrs.kneighbors(jd_tfidf)\n",
    "    names_similar = pd.Series(indices.flatten()).map(df3.reset_index()['File Name'])\n",
    "    similar_urls = pd.Series(indices.flatten()).map(df3.reset_index()['urls'])\n",
    "    result = pd.DataFrame({'Distance':distances.flatten(), 'Resume':names_similar, 'URLS' : similar_urls})\n",
    "    \n",
    "    return result[1:]\n",
    "fine_name=\"\"\n",
    "resume_list=[]\n",
    "resume_urls=[]\n",
    "def pre_process(event):\n",
    "    global fine_name\n",
    "    global resume_list\n",
    "    #resume_list1=[\"https://firebasestorage.googleapis.com/v0/b/capestone-945f7.appspot.com/o/0q1yyOoSTOQKSJt1yDeh%2FNithin_Tataundefined?alt=media&token=66cdc9a4-0ce7-4dc2-b7ab-0c3040e9a8fa\"]   \n",
    "    #resume_list1=[event[\"value\"]['fields'][\"url\"]['stringValue']]\n",
    "    resume_list1=event\n",
    "    global resume_urls\n",
    "    for i in resume_list1:\n",
    "        resume_urls.append(i[0])\n",
    "    #resume_list\n",
    "    for file_url in zip(resume_list1,range(0,len(resume_list1))):\n",
    "        response=requests.get(file_url[0])\n",
    "        import tempfile\n",
    "        fine_name=os.path.join(tempfile.gettempdir(),\"hello.pdf\")\n",
    "        #fine_name=str(\"hello.pdf\")\n",
    "        print(fine_name)\n",
    "        with open(fine_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Cpmpleted\")\n",
    "\n",
    "\n",
    "    resume_list = [] # stores all resumes\n",
    "    resume_list_pdf = [] # Captures files with pdf extension\n",
    "    resume_list_doc = [] # Captures files with doc extension\n",
    "    resume_list_docx = [] # Captures files with docx extension\n",
    "\n",
    "    file_names = [] # STORES RESUME FILE NAMES\n",
    "    job_desc_files = [] # stores jd paths\n",
    "\n",
    "    for i in resume_list1:\n",
    "        resume_list.append(\"hello.pdf\")\n",
    "    return resume_list                                   \n",
    "\n",
    "\n",
    "def hello_firestore(event, context):\n",
    "    pre= pre_process(event)                                     \n",
    "    ss=resume_df(resume_list, 'sample',event)\n",
    "    path_parts = context.resource.split('/documents/')[1].split('/')\n",
    "    collection_path = path_parts[0]\n",
    "    document_path = '/'.join(path_parts[1:])\n",
    "    affected_doc = client.collection(collection_path).document(document_path)\n",
    "    affected_doc.update({\n",
    "            u'resumeValue':ss[\"Distance\"]\n",
    "        })\n",
    "\n",
    "#pre= pre_process(event)   \n",
    "event={}\n",
    "event=[\"https://firebasestorage.googleapis.com/v0/b/capestone-945f7.appspot.com/o/0q1yyOoSTOQKSJt1yDeh%2FNithin_Tataundefined?alt=media&token=66cdc9a4-0ce7-4dc2-b7ab-0c3040e9a8fa\"]\n",
    "pre= pre_process(event)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bda4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0887256c8398904ea6b6eaee6d54d4304ae166627f28ad80ffdae267228d93255"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
